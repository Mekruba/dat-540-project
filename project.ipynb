{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition Project - DAT540 Introduction to Data Science\n",
    "*Authors: Haakon Vollheim Webb, Håkon Nodeland, Magnus Kjellesvig Egeland, Ninh Bao Turong, William Vagle*\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "We are tasked with recognizing hand gestures. We want to use machine learning tools to build an effective and reliable model. This model, along with the webcam of the computer running the code, will be used to create a live feed of the models interpretations of the hand gestures. \n",
    "\n",
    "The code has the following structure:\n",
    "\n",
    "0. Defining all Imports\n",
    "1. Downloading the Dataset\n",
    "2. Pre-Processing\n",
    "3. Sanity Check\n",
    "4. Defining a Model\n",
    "5. Defining Hyper Parameters\n",
    "6. Performing Cross Validation\n",
    "7. Configuring Webcam Access\n",
    "8. Predicting on a Single Frame\n",
    "9. Live Feed of Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from keras_tuner import Hyperband\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading the Dataset\n",
    "When running the code for the first time, we ensure that the user has the required dataset locally. If the dataset is not found locally, we download the it from kaggle and unzip it into /data/leapGestRecog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_folder = Path(\"./data/leapGestRecog\").expanduser()  # Adjust folder name as needed\n",
    "zip_path = Path(\"./data/archive.zip\").expanduser()\n",
    "\n",
    "if not dataset_folder.parent.exists():\n",
    "    print(f\"Creating the directory: {dataset_folder.parent}\")\n",
    "    dataset_folder.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if dataset folder exists\n",
    "if not dataset_folder.exists():\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        # Run shell command to download the dataset if it doesn't exist\n",
    "        !curl -L -o {zip_path} https://www.kaggle.com/api/v1/datasets/download/gti-upm/leapgestrecog\n",
    "    \n",
    "    # Unzip the downloaded file\n",
    "    if zip_path.exists():\n",
    "        print(\"Download complete. Extracting files...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_folder)  # Extracts to the folder above the zip file\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(\"Download failed. Please check your connection or Kaggle API credentials.\")\n",
    "else:\n",
    "    print(\"Dataset already exists locally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing\n",
    "The pre-processing is split into 3 parts: Normalization, data augmentation and data preperation.\n",
    "\n",
    "### Normalization\n",
    "We need to ensure that all images are of the same size, and that they are in gray scale. We use the OpenCV python binary extention loader (cv2) to read the images in grayscale:\n",
    "\n",
    "`img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)`\n",
    "\n",
    "And we use it to resize the images to the desired dimentions, to ensure that all images are of the same size and shape:\n",
    "\n",
    "`img_resized = cv2.resize(img, desired_size)`\n",
    "\n",
    "The `images` and `labels` lists contains the images and labels in the uint8-encoded format. These will be used for the data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list of images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# We only want to work with the first person's data (folder '00')\n",
    "base_folder = './data/leapGestRecog/leapGestRecog/00/'\n",
    "\n",
    "# Specify desired image size\n",
    "desired_size = (128, 128)\n",
    "\n",
    "if os.path.isdir(base_folder):\n",
    "    # For each gesture folder\n",
    "    for gesture_folder in os.listdir(base_folder):\n",
    "        gesture_path = os.path.join(base_folder, gesture_folder)\n",
    "        \n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(gesture_path):\n",
    "            \n",
    "            # Use gesture folder as label\n",
    "            label = gesture_folder \n",
    "            \n",
    "            # For each image in the gesture folder\n",
    "            for filename in os.listdir(gesture_path):\n",
    "                \n",
    "                # Check if the file is an image\n",
    "                if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                    \n",
    "                    # Read the image in grayscale and resize\n",
    "                    img_path = os.path.join(gesture_path, filename)\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    \n",
    "                    if img is not None:\n",
    "                        img_resized = cv2.resize(img, desired_size)\n",
    "                        \n",
    "                        # Keep the image in uint8 format with pixel values in [0, 255]\n",
    "                        images.append(img_resized)\n",
    "                        labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "The dataset only contains 200 images of each hand gesture. We want to increase this by creating duplicates with slight augmentation. Using Tensorflow's ImageDataGenerator, we can specify how much we want to augment the images. It will rotate, shift, zoom, flip, and modify the brightness randomly for each image. For the implementation, it is specified that we want `augments_per_image` number of augments. These augmented images are then saved locally.\n",
    "\n",
    "Missing: Check if augmented images already exist and how many copies, so as to not generate new augmented images every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images and labels to numpy arrays\n",
    "images = np.array(images, dtype=np.uint8)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Reshape images to add channel dimension (grayscale images)\n",
    "images = images.reshape((-1, desired_size[0], desired_size[1], 1))\n",
    "\n",
    "# Augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create a directory to save augmented images\n",
    "output_dir = 'data/augmented_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for label in np.unique(labels):\n",
    "    os.makedirs(f'{output_dir}/{label}', exist_ok=True)\n",
    "\n",
    "# Augment and collect augmented images and labels\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Number of augmentations per original image\n",
    "augmentations_per_image = 2\n",
    "\n",
    "# For each image\n",
    "for idx in range(len(images)):\n",
    "    img = images[idx]\n",
    "    label = labels[idx]\n",
    "    \n",
    "    # Reshape to (1, height, width, channels) which is the expected shape for the flow() function\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "    i = 0\n",
    "    for batch in datagen.flow(\n",
    "            img,\n",
    "            batch_size=1,\n",
    "            save_to_dir=output_dir+'/{}'.format(label),\n",
    "            save_prefix=f'{idx}',\n",
    "            save_format='jpeg'):\n",
    "        \n",
    "        # Append the augmented image and label to the lists\n",
    "        augmented_images.append(batch[0])\n",
    "        augmented_labels.append(label)\n",
    "        i += 1\n",
    "        if i >= augmentations_per_image:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "We want to combine the list of augmented images with the original samples. Additionally we need to add a channel dimenstion to the images, as it is a required field for Conv2D later on. Additionally we convert the labels to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and augmented data\n",
    "all_images = np.concatenate((images, np.array(augmented_images, dtype=np.uint8)))\n",
    "all_labels = np.concatenate((labels, np.array(augmented_labels)))\n",
    "\n",
    "# Adding a channel dimension to the images. The images are grayscale so the channel dimension is 1.\n",
    "# X here is a 4D array (number of samples, height, width, channels)\n",
    "# which we can directly use in a Conv2D layer in the next code block\n",
    "X = np.array(all_images).reshape(-1, desired_size[0], desired_size[1], 1)\n",
    "\n",
    "#Convert labels to integers (i.e. convert '01_palm' to (for instance) 0)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(all_labels)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_categorical = to_categorical(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sanity Check - Part 1\n",
    "To ensure we our implementation are working so far, we want to display some sample images and the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "classes = np.unique(y)\n",
    "fig, axes = plt.subplots(1, len(classes), figsize=(15, 5))\n",
    "for i, cls in enumerate(classes):\n",
    "    idx = y.tolist().index(cls)\n",
    "    axes[i].imshow(all_images[idx], cmap='gray')\n",
    "    axes[i].set_title(f\"Class: {le.inverse_transform([cls])[0]}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of classes, we can see that the classes are balanced\n",
    "# as we have 200 * augments_per_image samples for each class, all classes have the same number of samples\n",
    "sns.countplot(x=y)\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(ticks=classes, labels=le.inverse_transform(classes), rotation=45)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the model\n",
    "\n",
    "We are using a sequential model, which allows for a linear stack of layers. We are building a CNN, where \n",
    "\n",
    "The first layer is responsible for extracting basic features from the input image, reducing its spatial size, and preparing the data for deeper layers to learn higher-level features.\n",
    "\n",
    "The second layer is responsible for refining and combining the features extracted by the first layer to build a more abstract and detailed representation of the input data.\n",
    "\n",
    "The third layer, the flatten and dense layer, helps the network transition from feature extraction to learning the relationships necessary for classification.\n",
    "\n",
    "Finally we have the output layer which is responsible of producing the final classification output.\n",
    "\n",
    "\n",
    "All of the parameters defined in the `build_model(hp)` function are tunable. This is because we are using this function to perform the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    # Create a Sequential model. A sequential model is a linear stack of layers.\n",
    "    # It is a type of CNN model that is suitable for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    # First Convolutional Layer with tunable number of filters and kernel size\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('conv_1_filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        input_shape=(desired_size[0], desired_size[1], 1)\n",
    "    ))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second Convolutional Layer with tunable number of filters and kernel size\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('conv_2_filters', min_value=64, max_value=256, step=32),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten and Dense Layers with tunable dense units and dropout rate\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=64, max_value=256, step=32), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    \n",
    "    # Compile model with tunable learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Defining Hyper Parameters\n",
    "To find the hyper parameters, we use keras' Hyperband class. This class has the search() method, which performs a search for the best hyper parameters. This search is only required to be performed once. We can later use the found parameters to define a single model. Additionally, we will use these parameters for constructing the models during the Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuner and its parameters\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='tuning_dir',\n",
    "    project_name='hand_sign_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tuner search.\n",
    "# ETA: 2.5 hours, depending on the cpu/gpu, augments_per_image, max_epochs and factor.\n",
    "# Only necessary if you want to run the search again.\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_split=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Display the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Conv Layer 1 Filters: {best_hyperparameters.get('conv_1_filters')}\")\n",
    "print(f\"Conv Layer 1 Kernel Size: {best_hyperparameters.get('conv_1_kernel')}\")\n",
    "print(f\"Conv Layer 2 Filters: {best_hyperparameters.get('conv_2_filters')}\")\n",
    "print(f\"Conv Layer 2 Kernel Size: {best_hyperparameters.get('conv_2_kernel')}\")\n",
    "print(f\"Dense Units: {best_hyperparameters.get('dense_units')}\")\n",
    "print(f\"Dropout Rate: {best_hyperparameters.get('dropout')}\")\n",
    "print(f\"Learning Rate: {best_hyperparameters.get('learning_rate')}\")\n",
    "\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "# Convert one-hot encoded y_test back to labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Shows actual (y-axis) vs predicted values (x-axis)\n",
    "#The diagonal represents the correct predictions\n",
    "#The diagonal shows that the model never predicts false positives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross Validation\n",
    "\n",
    "To ensure that we have a good model, we want to cross validate it. This ensures us that the model reads unseed data in a good manner, that it is not over-fitted while also being a good process for evaluating the performance of the model. We are using the hyper parameters found in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y is in integer format (not one-hot encoded)\n",
    "# If y is one-hot encoded, convert it back to integer labels\n",
    "if y.ndim > 1:\n",
    "    y_labels = np.argmax(y, axis=1)\n",
    "else:\n",
    "    y_labels = y\n",
    "    \n",
    "#\n",
    "def build_model_with_parameters():\n",
    "\n",
    "    # Best Hyperparameters:\n",
    "    # Conv Layer 1 Filters: 128\n",
    "    # Conv Layer 1 Kernel Size: 3\n",
    "    # Conv Layer 2 Filters: 96\n",
    "    # Conv Layer 2 Kernel Size: 5\n",
    "    # Dense Units: 192\n",
    "    # Dropout Rate: 0.30000000000000004\n",
    "    # Learning Rate: 1e-05\n",
    "\n",
    "    # WEBB:\n",
    "    #Best Hyperparameters:\n",
    "    # Conv Layer 1 Filters: 32\n",
    "    # Conv Layer 1 Kernel Size: 5\n",
    "    # Conv Layer 2 Filters: 128\n",
    "    # Conv Layer 2 Kernel Size: 5\n",
    "    # Dense Units: 256\n",
    "    # Dropout Rate: 0.2\n",
    "    # Learning Rate: 0.0001\n",
    "    # 38/38 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.9333 - loss: 0.3256\n",
    "    # Test Accuracy: 0.9275\n",
    "\n",
    "    # Webb Best Hyperparameters\n",
    "    conv_1_filters = 32\n",
    "    conv_1_kernel = 5\n",
    "    conv_2_filters = 128\n",
    "    conv_2_kernel = 5\n",
    "    dense_units = 256\n",
    "    dropout_rate = 0.2\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # Best Hyperparameters\n",
    "    # conv_1_filters = 128\n",
    "    # conv_1_kernel = 3\n",
    "    # conv_2_filters = 96\n",
    "    # conv_2_kernel = 5\n",
    "    # dense_units = 192\n",
    "    # dropout_rate = 0.3\n",
    "    # learning_rate = 0.00001\n",
    "\n",
    "    \n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=conv_1_filters,\n",
    "                     kernel_size=(conv_1_kernel, conv_1_kernel),\n",
    "                     activation='relu',\n",
    "                     input_shape=(desired_size[0], desired_size[1], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=conv_2_filters,\n",
    "                     kernel_size=(conv_2_kernel, conv_2_kernel),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=dense_units, activation='relu'))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "K-fold cross-validation is a technique used in machine learning to evaluate the performance of a model. Here's how it works:\n",
    "\n",
    "1. **Split the Data**: The dataset is divided into `k` equally sized folds (subsets).\n",
    "2. **Training and Validation**: For each fold:\n",
    "   - Use `k-1` folds for training the model.\n",
    "   - Use the remaining 1 fold for validating the model.\n",
    "3. **Repeat**: This process is repeated `k` times, with each fold used exactly once as the validation data.\n",
    "4. **Average Performance**: The performance metric (e.g., accuracy, precision) is averaged over the `k` iterations to provide a more robust estimate of the model's performance.\n",
    "\n",
    "This method helps in reducing the variance of the performance estimate and ensures that every data point gets a chance to be in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_no = 1\n",
    "accuracy_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for train_index, val_index in skf.split(X, y_labels):\n",
    "    print(f'Fold {fold_no} -------------------------------------')\n",
    "\n",
    "    # Split data\n",
    "    X_train_cv, X_val_cv = X[train_index], X[val_index]\n",
    "    y_train_cv, y_val_cv = y[train_index], y[val_index]\n",
    "\n",
    "    # Ensure labels are one-hot encoded\n",
    "    y_train_categorical_cv = to_categorical(y_train_cv, num_classes=len(classes))\n",
    "    y_val_categorical_cv = to_categorical(y_val_cv, num_classes=len(classes))\n",
    "\n",
    "    # Build a fresh model for each fold\n",
    "    model = build_model_with_parameters()\n",
    "\n",
    "    # Train the model\n",
    "    history_cv = model.fit(X_train_cv, y_train_categorical_cv,\n",
    "                        epochs=20,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val_cv, y_val_categorical_cv),\n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(X_val_cv, y_val_categorical_cv, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} = {scores[0]:.4f}; {model.metrics_names[1]} = {scores[1]:.4f}')\n",
    "    accuracy_per_fold.append(scores[1])\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "\n",
    "print('-------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(len(accuracy_per_fold)):\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]:.4f} - Accuracy: {accuracy_per_fold[i]:.4f}%')\n",
    "print('-------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(accuracy_per_fold):.4f} (+- {np.std(accuracy_per_fold):.4f})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold):.4f}')\n",
    "print('-------------------------------------')\n",
    "\n",
    "\n",
    "# Score per fold\n",
    "# > Fold 1 - Loss: 0.0039 - Accuracy: 1.0000%\n",
    "# > Fold 2 - Loss: 0.0066 - Accuracy: 1.0000%\n",
    "# > Fold 3 - Loss: 0.0054 - Accuracy: 1.0000%\n",
    "# > Fold 4 - Loss: 0.0044 - Accuracy: 1.0000%\n",
    "# > Fold 5 - Loss: 0.0051 - Accuracy: 1.0000%\n",
    "# -------------------------------------\n",
    "# Average scores for all folds:\n",
    "# > Accuracy: 1.0000 (+- 0.0000)\n",
    "# > Loss: 0.0051\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with the saved hyperparameters\n",
    "# Only run this if you not already have a model.keras file locally\n",
    "model = build_model_with_parameters()\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Adjustments\n",
    "This code block defines functions to adjust incoming images from live feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the adjust_levels function\n",
    "def adjust_levels(channel, low_input, high_input, gamma, low_output, high_output):\n",
    "    # Ensure values are in [0,1]\n",
    "    channel = np.clip(channel, 0, 1)\n",
    "\n",
    "    # Initialize the output channel\n",
    "    out_channel = np.zeros_like(channel)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    denom = high_input - low_input\n",
    "    denom = denom if denom != 0 else 1e-6\n",
    "\n",
    "    # Apply levels adjustment\n",
    "    idx = (channel >= low_input) & (channel <= high_input)\n",
    "    x = (channel[idx] - low_input) / denom\n",
    "    x = x ** (1 / gamma)\n",
    "    out_channel[idx] = low_output + x * (high_output - low_output)\n",
    "\n",
    "    # For pixels below low_input\n",
    "    idx = channel < low_input\n",
    "    out_channel[idx] = low_output\n",
    "\n",
    "    # For pixels above high_input\n",
    "    idx = channel > high_input\n",
    "    out_channel[idx] = high_output\n",
    "\n",
    "    return out_channel\n",
    "\n",
    "\n",
    "# Adjusting the image to match the training data. \n",
    "def adjust_image(image, desired_size):\n",
    "    # Split the image into B, G, R channels\n",
    "    b, g, r = cv2.split(image)\n",
    "\n",
    "    # Adjust levels for each channel\n",
    "    # Red channel\n",
    "    r_adj = adjust_levels(r,\n",
    "                          low_input=0.078,\n",
    "                          high_input=0.88,\n",
    "                          gamma=1.0,\n",
    "                          low_output=0.0,\n",
    "                          high_output=1.0)\n",
    "\n",
    "    # Green channel\n",
    "    g_adj = adjust_levels(g,\n",
    "                          low_input=0.090,\n",
    "                          high_input=0.82,\n",
    "                          gamma=1.0,\n",
    "                          low_output=0.0,\n",
    "                          high_output=1.0)\n",
    "\n",
    "    # Blue channel\n",
    "    b_adj = adjust_levels(b,\n",
    "                          low_input=0.13,\n",
    "                          high_input=0.92,\n",
    "                          gamma=1.0,\n",
    "                          low_output=0.0,\n",
    "                          high_output=1.0)\n",
    "\n",
    "    # Merge the adjusted channels back into an image\n",
    "    image_adj = cv2.merge([b_adj, g_adj, r_adj])\n",
    "\n",
    "    # Convert image_adj back to uint8\n",
    "    image_adj_uint8 = np.clip(image_adj * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the adjusted image to HSV\n",
    "    image_hsv = cv2.cvtColor(image_adj_uint8, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Split the HSV channels\n",
    "    h, s, v = cv2.split(image_hsv)\n",
    "\n",
    "    # Convert V to float32 and normalize to [0,1]\n",
    "    v = v.astype(np.float32) / 255.0\n",
    "\n",
    "    # Adjust levels for the 'value' channel\n",
    "    v_adj = adjust_levels(v,\n",
    "                          low_input=0.21,\n",
    "                          high_input=0.58,\n",
    "                          gamma=0.23,\n",
    "                          low_output=0.0,\n",
    "                          high_output=0.85)\n",
    "\n",
    "    # Multiply by 255 and convert to uint8\n",
    "    v_adj_uint8 = np.clip(v_adj * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Merge the HSV channels back\n",
    "    hsv_adj = cv2.merge([h, s, v_adj_uint8])\n",
    "\n",
    "    # Convert back to BGR color space\n",
    "    image_final = cv2.cvtColor(hsv_adj, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    # Optionally reduce brightness\n",
    "    frame_darkened = cv2.convertScaleAbs(image_final, alpha=0.9, beta=0.3)\n",
    "\n",
    "    # Convert to grayscale for prediction\n",
    "    frame_gray = cv2.cvtColor(frame_darkened, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize the frame to match the input size\n",
    "    frame_resized = cv2.resize(frame_gray, desired_size)\n",
    "\n",
    "    # Return both the display image and the resized grayscale image\n",
    "    return frame_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Demonstration Section\n",
    "* Load Model\n",
    "* Run live video capture\n",
    "* Run prediction on frames taken from video capture\n",
    "* Provide accuracy on frames taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's input shape: (None, 128, 128, 1)\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "Saved frame to 'data/frames\\frame_0.jpg'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "Saved frame to 'data/frames\\frame_10.jpg'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "Saved frame to 'data/frames\\frame_20.jpg'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "Saved frame to 'data/frames\\frame_30.jpg'\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "frame_expanded shape: (1, 128, 128, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = load_model('test.keras')\n",
    "\n",
    "# Rebuild the LabelEncoder with the same labels used during training\n",
    "labels = ['01_palm', '02_l', '03_fist', '04_fist_moved', '05_thumb',\n",
    "          '06_index', '07_ok', '08_palm_moved', '09_c', '10_down']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "# Get the input shape of the model\n",
    "input_shape = model.input_shape  # Should be (None, 128, 128, 1)\n",
    "_, img_height, img_width, channels = input_shape\n",
    "\n",
    "print(f\"Model's input shape: {input_shape}\")\n",
    "\n",
    "# Specify desired image size\n",
    "desired_size = (img_width, img_height)\n",
    "\n",
    "# Start video capture from the default webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the directory for saving frames exists\n",
    "os.makedirs(\"data/frames\", exist_ok=True)\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to float32 and normalize to [0,1]\n",
    "    image = frame.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Copy the unmodified frame for display\n",
    "    frame_display = image.copy()\n",
    "    \n",
    "    # Adjust the image for prediction\n",
    "    frame_resized = adjust_image(image, desired_size)\n",
    "\n",
    "    # Prediction\n",
    "    frame_uint8 = frame_resized.astype(np.uint8)\n",
    "    frame_expanded = np.expand_dims(frame_uint8, axis=0)  # Add batch dimension\n",
    "    frame_expanded = np.expand_dims(frame_expanded, axis=-1)  # Add channel dimension\n",
    "\n",
    "    # Ensure data type is float32\n",
    "    frame_expanded = frame_expanded.astype(np.float32)\n",
    "\n",
    "    # Check the shape of the expanded frame\n",
    "    print(f\"frame_expanded shape: {frame_expanded.shape}\")  # Should be (1, 128, 128, 1)\n",
    "\n",
    "    # Save every n frames to debug or analyze\n",
    "    n = 10\n",
    "    if frame_count % n == 0:\n",
    "        frame_filename = os.path.join(\"data/frames\", f'frame_{frame_count}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame_resized)\n",
    "        print(f\"Saved frame to '{frame_filename}'\")\n",
    "    frame_count += 1\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(frame_expanded)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    confidence = np.max(predictions)\n",
    "\n",
    "    # Decode the predicted class index to get the gesture label\n",
    "    predicted_label = le.inverse_transform([predicted_class_index])[0]\n",
    "\n",
    "    # Overlay the prediction on the frame_display\n",
    "    cv2.putText(frame_display, f'Gesture: {predicted_label} ({confidence*100:.2f}%)',\n",
    "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Hand Gesture Recognition', frame_display)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check - Part 2\n",
    "* Live demonstration shows 100% accuracy, but we know it is not guessing correct.\n",
    "* Want to sanity check captured images with test data.\n",
    "* Can clearly seee difference in the images and why the result is incorrect.\n",
    "\n",
    "![Test Data](./data/leapGestRecog/leapGestRecog/00/01_palm/frame_00_01_0001.png)\n",
    "![Augmented Test Data](./path/to/image.png)\n",
    "![Captured Image](./path/to/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "* [Handwritten Digit Recognition using CNN with TensorFlow](https://learner-cares.medium.com/handwritten-digit-recognition-using-convolutional-neural-network-cnn-with-tensorflow-2f444e6c4c31)\n",
    "* ChatGPT\n",
    "* Co-Pilot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
